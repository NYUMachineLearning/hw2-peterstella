---
title: "Regression"
author: "Peter Stella"
date: "Fall 2019"
output:
  html_document:
    df_print: paged
fontsize: 11pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = T)
```

# Lab Section

In this lab, we will go over regression. We will be using the caret package in R. https://topepo.github.io/caret/train-models-by-tag.html

# Perfomance Metrics 

## Residual 

Deviation of the observed value to the estimated value (sample mean)
$$residual=y_i - \hat{y_i}$$
where $\hat{y_i}$ is the estimated value

## Mean Squared Error (MSE)

$$MSE=\frac{1}{n}\sum_{i=1}^{n}(y_i - \hat{y_i})^2$$

## Root Mean Squared Error (RMSE)
Same units as original data.

$$RMSE=\sqrt{MSE}$$

## L2 regularization : Ridge regression. Regularize by adding the sum of the coefficients, squared, to the function. 

$$Ridge Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p(w_j)^2$$

## L1 regularization : Lasso Regression. Regularize by adding the sum of the absolute value of the coefficients to the model. Coefficient estimates may be pushed to zero -- Lasso can perform variable selection

$$Lasso Regression=\sum_{i=1}^{n}(y_i - w_0 - \sum_{j=1}^{p}w_jx_{ij})^2 + \lambda\sum_{j=1}^p|w_j|$$


\newpage

### The broad steps of Machine learning in R. 

1. Split the data into training and test. Set test aside. 

2. Fit a good model to the training data. 

3. See how your model did on the training data.

4. Test how your model performs on the test data. 

# Regression

```{r, include=FALSE}
library(caret)
library(MASS)
library(ggplot2)
library(dplyr)
library(ggfortify)


#Mauna Loa CO2 concentrations
data(airquality)
```


1. Split data into training and test set (75% in train set, 25% in test set)

```{r}
  
airquality$group = sample.int(4,size = nrow(airquality), replace= TRUE)
train_regression = filter(airquality,group < 4)
test_regression = filter(airquality,group == 4)
```


### Linear Regression

* Assumes a linear relationship. 
* Independent variables should not be correlated (no mulitcollinearity)
* The number of observations should be greater than the number of independent variables.


$$RSS=\sum(y_i - \hat{y_i})^2$$
We will predict the response of the Temperature based on Wind. 

This is the data we will fit a linear model to. 
```{r}
ggplot(data = train_regression) +
geom_point(aes(x=Wind, y=Temp)) +
theme_bw()
```

2. Create and fit a linear model to predict Temperature from Wind using the training set

```{r}

linear_regression <- train( Temp ~ Wind , data=train_regression, method = "lm")
```

```{r}
coef <- linear_regression$finalModel$coefficients
coef[[1]]
coef[[2]]
```


3. Vizualize how your model performed on the train data by plotting the regression line on top of the train data points. 
```{r}
ggplot(train_regression, aes(Wind, Temp)) + geom_point() +geom_abline(intercept = coef[[1]], slope = coef[[2]])

```


4. Explore how the model performs on the test data. For Linear Regression:

* The residuals should be close to zero.
They are not, really. 
```{r}
summary(linear_regression$finalModel$residuals)

plot(linear_regression$finalModel$residuals)
```



* There should be equal variance around the regression line (homoscedasticity).
Plot 3 (scale/location) shows some heteroscedsaticity,i.e. not purely flat line

```{r}
plot(linear_regression$finalModel)

```

* Residuals should be normally distributed.
This is pretty much true (as the QQ plot shows)


* Independent variables and residuals should not be correlated.
They do not appear to be. 
```{r}
restrain <- train_regression[,c("Temp", "Wind")]
restrain$resid <- linear_regression$finalModel$residuals
ggplot(restrain, aes(resid, Wind)) + geom_point() +geom_smooth(method =lm)
```


4 a) See how the model performs on the test data
```{r}

linear_predict <- predict(linear_regression, newdata= test_regression)
testvspredict <- cbind(test_regression, linear_predict)
testvspredict <- testvspredict %>% mutate(resid= Temp - linear_predict)
```

4 b) Look at the residuals. Are they close to zero?
Not really
```{r}
summary(testvspredict$resid)
```


4 c) Plot predicted temperature vs observed temperature. A strong model should show a strong correlation
```{r}
ggplot(testvspredict, aes(linear_predict, Temp)) + geom_point() +geom_smooth(method =lm)
```

4 d) Visualize the predicted values in relation to the real data points. Look for homoscedasticity



```{r}
# probably a stupid way of doing this, but easier for me to rename variables and then just change data sources in ggplot
linear_predict2 <- test_regression %>% select(Wind)
linear_predict2 <- cbind(linear_predict2, linear_predict)
colnames(linear_predict2)[2] <- "Temp"

# Extract coefficients from the model
# plot the regression line on the predicted values
# plot the original test values

ggplot(test_regression, aes(Wind, Temp)) + geom_point() +geom_abline(intercept = coef[[1]], slope = coef[[2]]) +
geom_point(data=linear_predict2, color= "red")


```

4 e) Residuals should be normally distributed. Plot the density of the residuals
```{r}
residuals_lin <- residuals(linear_regression)
ggplot(data=testvspredict) +
 geom_density(aes(resid))
```


4 f) Independent variables and residuals should not be correlated
```{r}
cor.test(train_regression$Wind, resid(linear_regression))
```


### Linear Regression with Regularization

5. Create a linear model using L1 or L2 regularization to predict Temperature from Wind and Month variables. Plot your predicted values and the real Y values on the same plot. 
```{r}
library(glmnet)
library(Matrix)

```

I found this rather difficult to do in caret, but here I set a bunch of possible lambda values

```{r}
set.seed(123)
lambda <- 10^seq(-3, 3, length = 100)
```

and feed those to caret using glmnet for ridge (l2) regularization, outputting my final regression coefficients
```{r}
multi_regression <- train( Temp ~ (Month + Wind) , data=train_regression, method = "glmnet",trControl = trainControl("cv", number = 10), tuneGrid = expand.grid(alpha = 0, lambda = lambda))
coef(multi_regression$finalModel, multi_regression$bestTune$lambda)
```

predict test values, generate residuals, althoguh not used here

```{r}
multiple_predict <- predict(multi_regression, newdata= test_regression)
multivspredict <- cbind(test_regression, multiple_predict)
multivspredict <- multivspredict %>% mutate(resid= Temp - multiple_predict)
```

A very messy bit of wrangling to build a simple dataframe with temperature, wind and month data. I have recoded the "group" variable that I originally used to separate real from predicted data (predicted points are now group 1)
```{r}
multiple_predict2 <- test_regression %>% select(Wind, Month, group)
multiple_predict2 <- cbind(multiple_predict2, multiple_predict)
colnames(multiple_predict2)[4] <- "Temp"
multiple_predict2 <- multiple_predict2 %>% mutate(group = replace(group,, 1))
test_regression2 <- test_regression %>% select(Wind, Month, Temp, group)
multiple_predict2 <- rbind(multiple_predict2, test_regression2)


```

I wasn't sure how you wanted me to plot the data here, but I thought faceting by month was probably the most informative. 

```{r}
p <- ggplot(multiple_predict2, aes(Wind, Temp)) + geom_point(aes(color=group))
p+facet_grid(cols= vars(Month))
```

